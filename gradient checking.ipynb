{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient check for linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm is equal to: 1.94882138933e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(loc=0.5,scale=0.5,size = (50,20))\n",
    "input_grad = np.eye(20)\n",
    "weights = np.random.normal(size=(20,20))\n",
    "bias = np.random.normal(20)\n",
    "\n",
    "def forward(X):\n",
    "    Y = X.dot(weights.T) + bias\n",
    "    return Y\n",
    "\n",
    "def backward(input_grad):\n",
    "    return input_grad.dot(weights)\n",
    "\n",
    "eps = 1e-4\n",
    "num_grad = np.zeros(shape=(Y.shape[1],X.shape[1]))\n",
    "for l in xrange(X.shape[0]):\n",
    "    num_grad_per_object = np.zeros(shape=(X.shape[1],Y.shape[1]))\n",
    "    for i in xrange(Y.shape[1]):\n",
    "        for j in xrange(X.shape[1]):\n",
    "            X[l,j] += eps\n",
    "            Y_plus = forward(X)\n",
    "            X[l,j] -= 2*eps\n",
    "            Y_minus = forward(X)\n",
    "            X[l,j] += eps\n",
    "            num_grad_per_object[i,j] = ((Y_plus[l,i] - Y_minus[l,i])/(2.*eps))\n",
    "    num_grad += (1./X.shape[0])*num_grad_per_object\n",
    "\n",
    "    \n",
    "print \"Frobenius norm is equal to:\",np.linalg.norm(num_grad - backward(input_grad))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Gradient check for other \"Hadamar product\"-layers (sigmoid, tanh,relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "def forward(x):\n",
    "    return sigmoid(x)\n",
    "\n",
    "def backward(x,input_grad):\n",
    "    return input_grad*sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8727581533706499e-09"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(loc=0.5,scale=0.5,size = (50,30))\n",
    "input_grad = np.ones(shape=X.shape)\n",
    "eps = 1e-4 \n",
    "\n",
    "num_grad = np.zeros(shape=X.shape)\n",
    "for i in xrange(X.shape[0]):\n",
    "    for j in xrange(X.shape[1]):\n",
    "        X[i,j] += eps\n",
    "        Y_plus = forward(X)\n",
    "        X[i,j] -= 2*eps\n",
    "        Y_minus = forward(X)\n",
    "        X[i,j] += eps\n",
    "        num_grad[i,j] = (Y_plus[i,j] - Y_minus[i,j])/(2.*eps)\n",
    "\n",
    "np.linalg.norm(num_grad - backward(X,input_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=1,keepdims=True)\n",
    "\n",
    "def forward(x):\n",
    "    return softmax(x)\n",
    "\n",
    "def backward(x,input_grad):\n",
    "    return input_grad*softmax(x) - softmax(x)*np.sum(input_grad*softmax(x),axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(loc=0.5,scale=0.5,size = (50,30))\n",
    "eps = 1e-4 \n",
    "\n",
    "num_grad = np.zeros(shape=(X.shape[1],X.shape[0],X.shape[1]))\n",
    "an_grad = np.zeros(shape=(X.shape[1],X.shape[0],X.shape[1]))\n",
    "\n",
    "for i in xrange(X.shape[1]):\n",
    "    input_grad = np.zeros(shape=X.shape)\n",
    "    input_grad[:,i] = np.ones(shape=input_grad.shape[0])\n",
    "    an_grad[i,:,:] = backward(X,input_grad)\n",
    "    for l in xrange(X.shape[0]):\n",
    "        for j in xrange(X.shape[1]):\n",
    "            X[l,j] += eps\n",
    "            Y_plus = forward(X)\n",
    "            X[l,j] -= 2*eps\n",
    "            Y_minus = forward(X)\n",
    "            num_grad[i,l,j] = (Y_plus[l,i] - Y_minus[l,i])/(2.*eps)\n",
    "            X[l,j] += eps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for MSE, CrossEntropy and NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(X,T):\n",
    "    return -np.sum(T*np.log(X))\n",
    "def backward(X,T):\n",
    "    return -T/X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(low=1e-8,high = 1.0,size = (50,30))\n",
    "T = 1.*(X>=np.max(X,axis=1,keepdims=True))\n",
    "eps = 1e-4 \n",
    "\n",
    "num_grad = np.zeros(shape=(X.shape[0],X.shape[1]))\n",
    "\n",
    "for i in xrange(X.shape[0]):\n",
    "    for j in xrange(X.shape[1]):\n",
    "        X[i,j] += eps\n",
    "        Y_plus = forward(X,T)\n",
    "        X[i,j] -= 2*eps\n",
    "        Y_minus = forward(X,T)\n",
    "        X[i,j] += eps\n",
    "        num_grad[i,j] = (Y_plus - Y_minus)/(2.*eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6163998429313549e-08"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_grad = backward(X,T)\n",
    "np.linalg.norm(num_grad - an_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.267221358828609"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
